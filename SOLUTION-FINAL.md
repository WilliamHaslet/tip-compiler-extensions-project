# Final Deliverable

William Haslet\
Ruthvik Gajjala

## Testing Setup

To get started with this deliverable, we decided on a couple things beforehand in order to assess the benefits of the optimizations we were adding. First and foremost, we initially decided that we would be using the runtime of the test files as the benchmark for the additional optimization passes. With runtime, we decided that the number of iterations that we would be running the test files would be variable, as some optimizations exploit and show larger performance gains than others. With this in mind, the number of iterations that we ran test files ranged from 50,000 to 30,000,000, depending on the particular pass we were looking at. We did soon realize that running time is not the sole indicator of performance gain, and some of the optimizations we included showed little to no improvement in runtime; however, these optimizations did indeed show clear improvements in the llvm bitcode structure. Thus, we eventually decided that our benchmark for assessing optimizations would vary between running time and the llvm structure as we will talk about below. To facilitate testing, we used a bash script that detailed the number of iterations that the test file should be run, the specific test file, and which optimizations to remove at the time of testing. We timed this three times, and took the average when analyzing results. If these results were inconclusive, we then dove into the llvm bitcode itself to assess whether the optimization pass actually provided benefit, or if we should remove it from our compiler. In addition to the bash script, we also added various command line options to the build script to perform an ablation study with our optimizations. First, we altered the -do optimization to take a specific string as a parameter, and this string determined which function pass not to include when timing the executable. Through this method, we were able to acutely assess the individual benefit of a certain optimization pass. In addition, we also added command line options -df and -dm that disable function passes and module passes respectively. Finally, we included option -or to specify the number of rounds that optimizations should be run. For each of the included optimizations, there is a corresponding test that demonstrates the power of the optimization, located in the ./test/optimization directory we created. 

## Function Passes

In total, our optimizer has 11 total function passes. Each one provides a unique benefit, whether it be through clear improvements in runtime, or improvements in the llvm bitcode structure. Below, we detail each pass as well as what performance benefit it brought through our ablation study and the justification for keeping it in our optimizer. Our phase ordering scheme started with the given five optimizations, followed by various loop optimizations, finally followed by sinking/jump optimizations.

### Promote Memory to Register (createPromoteMemoryToRegisterPass):

This pass brought by far the largest improvement in terms of performance, both in our small tests and our big bang test. When testing it against our base.tip test, it reduced the number of instructions in the function from 92 to 40 in the perf() function, which was doing all of the work in this test. Despite not showing a huge improvement in runtime, it was clear that reduction in the number of llvm instructions (particularly the number of load instructions) made it worth keeping. In addition, when testing this optimization against the load.tip test, we saw a reduction in run time for 20,000,000 iterations of the test from 0.36 seconds without the optimization, to 0.15 seconds with the optimization; this is nearly a 60% decrease. We believe this is primarily due to the huge reduction in alloca and load statements that the optimization causes.

### Instruction Combining (createInstructionCombiningPass):

Due to the number of similar passes to instruction combining that were present in the optimizer, the true benefit of this pass was often hidden by other optimizations. However, we were able to see in our big bang test that when specifically removing this pass, we saw a run time increase of 0.04 seconds, which is non-trivial. Thus, we thought there was enough justification that instruction combining was not being dominated by the other optimizations, and the ablation study proved this.

### Reassociation (createReassociationPass):

This pass also showed huge gains in certain tests, mainly due to its ability to use mathematical properties to exploit further optimizations. In the rep.tip test running 20,000,000 times, there was a runtime decrease from 0.11 seconds to 0.6 seconds, which is a huge margin. The rep.tip test repeatedly uses similar expressions across many different local variables, which is why we believe the reassociation pass had huge benefit in this case. After looking into this pass and observing the llvm bitcode changes, we found that there was more than enough justification for including it in our optimizer.

### GVN (createGVNPass):

Upon researching this pass, we found that it may make the same optimizations as instruction combining, which made us wary of including it. Additionally, since we were already using the promote memory to registers pass, we were concerned that the full capability of using GVN to eliminate redundant load instructions would not be fully utilized. This held true when we tried ablation testing this optimization on the obviously redundant instructions found within combine.tip, which showed no running time benefit (likely due to some of the above passes already making optimizations). Nonetheless, adding this optimization to our big bang test showed a 0.01 second reduction in runtime, so we decided that there was enough justification to keep it.

### CFG Simplification (createCFGSimplificationPass):

In testing this pass, we found that some of the latter passes that we added actually hid some of the initial improvements we had seen in testing. To replicate this, we tested this pass once again in concert with the initial five included optimizations to observe the benefit. Specifically, we used the unreach.tip file, which includes a block of unreachable code. As expected, the inclusion of this optimization removed the branching associated with the unreachable code, greatly reducing the llvm bitcode generated. Additionally, there was a slight runtime decrease from 0.06 seconds to 0.05 seconds on 20,000,000 iterations. This is evidence that the unreachable condition is never checked, and we felt this was enough to include this pass in our optimizer.

Next are the passes that we added to the optimizer.

### Loop Invariant Code Motion (createLICMPass):

Since loops and branches are some of the biggest potential exploits for optimization, we added various different loop optimizations, starting the loop invariant code motion. As seen in the licm.tip test file, there is a series of very expensive loop invariant instructions in the mul() function. When adding the licm pass to this test, all of these instructions were immediately hoisted out of the branch body for huge improvements. On 100,000 iterations of the test file, there was a reduction from 0.69 seconds to 0.53 seconds, evidencing that the code motion is indeed doing something and is worth keeping.

### Dead Loop Deletion (createLoopDeletionPass):

While the CFGSimplification is capable of detecting unreachable blocks, we realized that it is not necessarily capable of removing dead loops that donâ€™t do anything in the program. Thus, we decided to test this pass using the deadloop.tip test file, which includes a simple for-loop that has no actual purpose. Without this optimization (and the test file being run 1,000,000 times), the run time was 0.26 seconds; with the optimization, the run time was reduced astronomically to 0.00 seconds due to the loop completely being eliminated from the llvm bitcode.

### Loop Rotate (createLoopRotatePass):

This pass essentially converts a loop into a do-while by hoisting body code into the header to avoid expensive branching statements if at all possible. Surprisingly, this optimization actually showed significant time reductions when run on various different test files, and this could possibly be due to alterations also happening in the main() function of the test file which controls the number of iterations. In the rotate.tip test file run 1,000,000 times, the inclusion of the loop rotate pass reduced the run time from 0.28 seconds to 0.14 seconds.

### Loop Unroll (createLoopUnrollPass):

We found that this pass often worked in concert with the loop rotate pass, and allowed very good performance improvements in specific test cases. In the unroll.tip file, which is very similar in nature to rotate.tip, running the test file 1,000,000 times saw a reduction from 0.16 seconds to 0.00 seconds. In this case, the loop was completely eliminated from the llvm bitcode. It is important to note however that loop unrolling may put a strain on memory, and can only be truly utilized in niche cases in the way that we implemented our sip constructs. However, we still decided to keep it in case these situations arise.

### Code Sinking (createSinkingPass):

This pass attempts to push code as far down the cfg as possible in the case in which common expressions exist. This was one of two passes that we added that did not show a clear performance gain in terms of running time, but did show that the altered llvm was more concise in its representation. The test file used for this pass was sink.tip. When looking at the bitcode generated as a result of this optimization, we saw that some of the common expressions in the if-else blocks were pushed elsewhere. We thought that this rearranging of llvm structure could be exploited by other passes, such as loop rotation, so we decided to keep it.

### Jump Threading (createJumpThreadingPass): 

This pass does something very similar to code sinking, and we were curious to see if any major differences could be seen between the two. Once again, we decided to apply this optimization to the sink.tip file, and while we did not see any clear running time changes, there was a very interesting observation in the llvm bitcode. The number of headers in the perf() function had actually been reduced by one, from 13 to 12, upon applying this optimization (even when code sinking had already been applied). Since branching is a very expensive operation, we believe that this change in bitcode is for the better, and could potentially reduce the number of branch instructions needed as the number of iterations grows. 

## IPO Passes

The first interprocedural optimization pass we added was dead argument elimination, which removes unused arguments for all the functions in the module. This optimization had the most obvious speed influence on the improvement in run time. The test file has two functions with seven arguments that the functions only use one of. Running 30,000,000 loops of the functions with optimization caused an improvement from a 0.79 seconds run time to 0.45 seconds. The next interprocedural pass we added was a merge function pass, which combines similar functions into the same bitcode. The merge function pass gave a much less impressive result than dead argument elimination, with an improvement from 0.54 seconds to 0.52 seconds for 20,000,000 runs on a test program with some identical functions for the pass to collapse together. The next interprocedural pass we added was hot cold splitting, which should separate code blocks into hot and cold categories to improve cache performance. With 1,000,000 runs of our test file, we saw an improvement from 1.07 run time to 1.06 run time. Next we added the partial inlining pass, which should inline parts of functions. The test file is a program that loops over functions that have simple instructions for the pass to pull out and inline. Over 30,000,000 runs of the test file, the run time improved from 0.98 to 0.95. The last interprocedural optimization pass we added is function inlining, which should perform a similar optimization to the partial inlining pass but inline entire functions instead of just parts of them. We looked at adding one more interprocedural optimization, which was the IPSCCP pass. This pass is supposed to propagate constants from call sites into the bodies of functions, but it showed a consistent slowdown of 0.05 seconds over 50,000,000.

## Big Bang Test

For our big bang test, we combined elements of the tests that were focused on a single optimization into a single file. First, we tested the big bang test program with all of our optimizations against a run with no optimizations. Over 10,000,000 runs, there was an improvement from 12.01 seconds to 4.18 seconds with our optimizations. Next we tried removing specific optimizations and running 1,000,000 loops of the test file. For the functional optimizations, the most influential optimization was to promote memory to register pass, which resulted in a loss of 0.38 seconds when it was removed. The peephole optimizations resulted in about 0.04 seconds of improvement. Reassociation, common subexpression elimination, instruction sinking, and control flow simplification all resulted in an improvement of about 0.01 seconds. Loop unrolling resulted in an improvement of 0.24 seconds. For the interprocedural optimizations, function inlining had the most impact, with an improvement of 0.04 seconds, while the rest of the interprocedural optimizations only improved the run time by about 0.01 seconds.

## Why We Decided To Stop Adding Passes

Initially, we looked over the IPO.h and Scalar.h for any optimization passes that seemed interesting. We tried adding any that caught our attention, and kept them in Optimize.cpp if they showed some improvement to the run time of our test file or a smaller bitcode length output. Once we implemented all the passes that we initially noticed, we took a second into the other available passes in the llvm directory to see if there were any optimization possibilities that we missed. Once we had about ten new passes implemented, any more that we tried adding had no effect on bitcode output size or improvements to the run time. This is most likely because the effect of the new optimizations were already being covered by the other passes we added or the passes that were already present in the optimizer. At this point, there was no longer a need to add more optimization passes, so we decided to stop.
